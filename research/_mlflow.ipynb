{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'g:\\\\'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('../')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"MLFLOW_TRACKING_URI\"] = 'https://dagshub.com/nikhil0035/Machine_Translation_using_Transformers.mlflow'\n",
    "os.environ[\"MLFLOW_TRACKING_USERNAME\"] = 'nikhil0035'\n",
    "os.environ[\"MLFLOW_TRACKING_PASSWORD\"] = '8d6d437e619e46ffbd80ee64bee271c7f105427c'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.Translate.components.dataset import BilingualDataset,causal_mask\n",
    "from src.Translate.components.build_model import Prepare_model\n",
    "from src.Translate.utils.common import *\n",
    "from src.Translate.entity.config_entity import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext.datasets as datasets\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "import mlflow\n",
    "\n",
    "from urllib.parse import urlparse\n",
    "import torchmetrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.Translate.config.configuration import ConfigurationManager\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Evaluation:\n",
    "    def __init__(self,config:Config_Data,params:TrainingConfig,eval:EvaluationConfig) -> None:\n",
    "        self.config = config\n",
    "        self.params = params\n",
    "        self.eval = eval\n",
    "    \n",
    "    @staticmethod\n",
    "    def greedy_decode(model, source, source_mask, tokenizer_src, tokenizer_tgt, max_len, device):\n",
    "        sos_idx = tokenizer_tgt.token_to_id('[SOS]')\n",
    "        eos_idx = tokenizer_tgt.token_to_id('[EOS]')\n",
    "\n",
    "        # Precompute the encoder output and reuse it for every step\n",
    "        encoder_output = model.encode(source, source_mask)\n",
    "        # Initialize the decoder input with the sos token\n",
    "        decoder_input = torch.empty(1, 1).fill_(sos_idx).type_as(source).to(device)\n",
    "        while True:\n",
    "            if decoder_input.size(1) == max_len:\n",
    "                break\n",
    "\n",
    "            # build mask for target\n",
    "            decoder_mask = causal_mask(decoder_input.size(1)).type_as(source_mask).to(device)\n",
    "\n",
    "            # calculate output\n",
    "            out = model.decode(encoder_output, source_mask, decoder_input, decoder_mask)\n",
    "\n",
    "            # get next token\n",
    "            prob = model.project(out[:, -1])\n",
    "            _, next_word = torch.max(prob, dim=1)\n",
    "            decoder_input = torch.cat(\n",
    "                [decoder_input, torch.empty(1, 1).type_as(source).fill_(next_word.item()).to(device)], dim=1\n",
    "            )\n",
    "\n",
    "            if next_word == eos_idx:\n",
    "                break\n",
    "\n",
    "        return decoder_input.squeeze(0)\n",
    "    \n",
    "    def run_validation(self, model, validation_ds, tokenizer_src, tokenizer_tgt):\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(\"Using device:\", device)\n",
    "        self.model = model\n",
    "        model = self.model.to(device)\n",
    "        model.eval()\n",
    "        count = 0\n",
    "        max_len = self.config.seq_len\n",
    "        source_texts = []\n",
    "        expected = []\n",
    "        predicted = []\n",
    "\n",
    "\n",
    "        with torch.no_grad():\n",
    "            batch_iterator = tqdm(validation_ds)\n",
    "            for batch in batch_iterator:\n",
    "                count += 1\n",
    "                encoder_input = batch[\"encoder_input\"].to(device) # (b, seq_len)\n",
    "                encoder_mask = batch[\"encoder_mask\"].to(device) # (b, 1, 1, seq_len)\n",
    "\n",
    "                # check that the batch size is 1\n",
    "                assert encoder_input.size(\n",
    "                    0) == 1, \"Batch size must be 1 for validation\"\n",
    "\n",
    "                model_out = self.greedy_decode(model, encoder_input, encoder_mask, tokenizer_src, tokenizer_tgt, max_len, device)\n",
    "\n",
    "                source_text = batch[\"src_text\"][0]\n",
    "                target_text = batch[\"tgt_text\"][0]\n",
    "                model_out_text = tokenizer_tgt.decode(model_out.detach().cpu().numpy())\n",
    "\n",
    "                source_texts.append(source_text)\n",
    "                expected.append(target_text)\n",
    "                predicted.append(model_out_text)\n",
    "\n",
    "                if count ==5:\n",
    "                    break\n",
    "\n",
    "        metric = torchmetrics.CharErrorRate()\n",
    "        self.cer = metric(predicted, expected)\n",
    "       \n",
    "\n",
    "        # Compute the word error rate\n",
    "        metric = torchmetrics.WordErrorRate()\n",
    "        self.wer = metric(predicted, expected)\n",
    "      \n",
    "\n",
    "        # Compute the BLEU metric\n",
    "        metric = torchmetrics.BLEUScore()\n",
    "        self.bleu = metric(predicted, expected)\n",
    "\n",
    "    def log_into_mlflow(self):\n",
    "        mlflow.set_registry_uri(self.eval.mlflow_uri)\n",
    "        tracking_url_type_store = urlparse(mlflow.get_tracking_uri()).scheme\n",
    "\n",
    "        with mlflow.start_run():\n",
    "            mlflow.log_params(self.eval.all_params)\n",
    "\n",
    "            mlflow.log_metrics(\n",
    "                {\"CER\": self.cer, \"WER\": self.wer , \"Bleu\":self.bleu}\n",
    "            )\n",
    "\n",
    "            if tracking_url_type_store != \"file\":\n",
    "\n",
    "                # Register the model\n",
    "                # There are other ways to use the Model Registry, which depends on the use case,\n",
    "                # please refer to the doc for more information:\n",
    "                # https://mlflow.org/docs/latest/model-registry.html#api-workflow\n",
    "                mlflow.pytorch.log_model(self.model, \"model\", registered_model_name=\"NMT_Transformers\")\n",
    "            else:\n",
    "                mlflow.pytorch.log_model(self.model, \"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'config\\\\config.yaml'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[47], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mTranslate\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpipeline\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstage_01_data_injestion\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataIngestionTrainingPipeline\n\u001b[0;32m      2\u001b[0m data_obj \u001b[38;5;241m=\u001b[39m DataIngestionTrainingPipeline()\n\u001b[1;32m----> 3\u001b[0m train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt \u001b[38;5;241m=\u001b[39m \u001b[43mdata_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mg:\\machine_translation_using_transformers\\src\\Translate\\pipeline\\stage_01_data_injestion.py:17\u001b[0m, in \u001b[0;36mDataIngestionTrainingPipeline.main\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmain\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m---> 17\u001b[0m     config_instance \u001b[38;5;241m=\u001b[39m \u001b[43mConfigurationManager\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m     config_obj \u001b[38;5;241m=\u001b[39m config_instance\u001b[38;5;241m.\u001b[39mget_config()\n\u001b[0;32m     19\u001b[0m     param_obj \u001b[38;5;241m=\u001b[39m config_instance\u001b[38;5;241m.\u001b[39mget_training_config()\n",
      "File \u001b[1;32mg:\\machine_translation_using_transformers\\src\\Translate\\config\\configuration.py:12\u001b[0m, in \u001b[0;36mConfigurationManager.__init__\u001b[1;34m(self, config_filepath, params_filepath)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m      9\u001b[0m     config_filepath \u001b[38;5;241m=\u001b[39m CONFIG_FILE_PATH,\n\u001b[0;32m     10\u001b[0m     params_filepath \u001b[38;5;241m=\u001b[39m PARAMS_FILE_PATH):\n\u001b[1;32m---> 12\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig \u001b[38;5;241m=\u001b[39m \u001b[43mread_yaml\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig_filepath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams \u001b[38;5;241m=\u001b[39m read_yaml(params_filepath)\n\u001b[0;32m     15\u001b[0m     create_directories([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39martifacts_root])\n",
      "File \u001b[1;32mc:\\Users\\nikhi\\anaconda3\\envs\\NMT\\lib\\site-packages\\ensure\\main.py:849\u001b[0m, in \u001b[0;36mWrappedFunctionReturn.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    841\u001b[0m         msg \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    842\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArgument \u001b[39m\u001b[38;5;132;01m{arg}\u001b[39;00m\u001b[38;5;124m of type \u001b[39m\u001b[38;5;132;01m{valt}\u001b[39;00m\u001b[38;5;124m to \u001b[39m\u001b[38;5;132;01m{f}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    843\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdoes not match annotation type \u001b[39m\u001b[38;5;132;01m{t}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    844\u001b[0m         )\n\u001b[0;32m    845\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m EnsureError(msg\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    846\u001b[0m             arg\u001b[38;5;241m=\u001b[39marg, f\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf, t\u001b[38;5;241m=\u001b[39mtempl, valt\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mtype\u001b[39m(value)\n\u001b[0;32m    847\u001b[0m         ))\n\u001b[1;32m--> 849\u001b[0m return_val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    850\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(return_val, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_templ):\n\u001b[0;32m    851\u001b[0m     msg \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    852\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReturn value of \u001b[39m\u001b[38;5;132;01m{f}\u001b[39;00m\u001b[38;5;124m of type \u001b[39m\u001b[38;5;132;01m{valt}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    853\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdoes not match annotation type \u001b[39m\u001b[38;5;132;01m{t}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    854\u001b[0m     )\n",
      "File \u001b[1;32mg:\\machine_translation_using_transformers\\src\\Translate\\utils\\common.py:35\u001b[0m, in \u001b[0;36mread_yaml\u001b[1;34m(path_to_yaml)\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myaml file is empty\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m---> 35\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[1;32mg:\\machine_translation_using_transformers\\src\\Translate\\utils\\common.py:28\u001b[0m, in \u001b[0;36mread_yaml\u001b[1;34m(path_to_yaml)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"reads yaml file and returns\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \n\u001b[0;32m     17\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;124;03m    ConfigBox: ConfigBox type\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 28\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpath_to_yaml\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m yaml_file:\n\u001b[0;32m     29\u001b[0m         content \u001b[38;5;241m=\u001b[39m yaml\u001b[38;5;241m.\u001b[39msafe_load(yaml_file)\n\u001b[0;32m     30\u001b[0m         logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myaml file: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_to_yaml\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m loaded successfully\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'config\\\\config.yaml'"
     ]
    }
   ],
   "source": [
    "from Translate.pipeline.stage_01_data_injestion import DataIngestionTrainingPipeline\n",
    "data_obj = DataIngestionTrainingPipeline()\n",
    "train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt = data_obj.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-02-10 13:04:17,976: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
      "[2024-02-10 13:04:17,978: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2024-02-10 13:04:17,979: INFO: common: created directory at: artifacts]\n"
     ]
    }
   ],
   "source": [
    "from Translate.pipeline.stage_02_prepare_model import PrepareModelPipeline\n",
    "prepare_model_obj = PrepareModelPipeline()\n",
    "model = prepare_model_obj.main(tokenizer_src.get_vocab_size(), tokenizer_tgt.get_vocab_size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_filename = latest_weights_file_path(config_obj)\n",
    "state = torch.load(model_filename)\n",
    "model.load_state_dict(state['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-02-10 13:05:15,626: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
      "[2024-02-10 13:05:15,629: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2024-02-10 13:05:15,631: INFO: common: created directory at: artifacts]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    config_instance = ConfigurationManager()\n",
    "    config_obj = config_instance.get_config()\n",
    "    param_obj = config_instance.get_training_config()\n",
    "    eval_obj = config_instance.get_evaluation_config()\n",
    "\n",
    "   \n",
    "\n",
    "except Exception as e:\n",
    "   raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "Evaluation_obj =Evaluation(config=config_obj,params=param_obj,eval=eval_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 4/3234 [00:35<7:51:28,  8.76s/it]\n"
     ]
    }
   ],
   "source": [
    "Evaluation_obj.run_validation(model,val_dataloader, tokenizer_src, tokenizer_tgt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/02/10 13:33:25 WARNING mlflow.utils.requirements_utils: Found torch version (2.0.1+cu118) contains a local version label (+cu118). MLflow logged a pip requirement for this package as 'torch==2.0.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "2024/02/10 13:33:41 WARNING mlflow.utils.requirements_utils: Found torch version (2.0.1+cu118) contains a local version label (+cu118). MLflow logged a pip requirement for this package as 'torch==2.0.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "c:\\Users\\nikhi\\anaconda3\\envs\\NMT\\lib\\site-packages\\_distutils_hack\\__init__.py:33: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n",
      "Successfully registered model 'NMT_Transformers'.\n",
      "2024/02/10 13:35:19 INFO mlflow.tracking._model_registry.client: Waiting up to 300 seconds for model version to finish creation.                     Model name: NMT_Transformers, version 1\n",
      "Created version '1' of model 'NMT_Transformers'.\n"
     ]
    }
   ],
   "source": [
    "Evaluation_obj.log_into_mlflow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NMT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
