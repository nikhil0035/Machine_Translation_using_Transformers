{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class Config_Data:\n",
    "    batch_size: int\n",
    "    num_epochs: int\n",
    "    lr: float\n",
    "    seq_len: int\n",
    "    d_model: int\n",
    "    datasource: str\n",
    "    lang_src: str\n",
    "    lang_tgt: str\n",
    "    model_folder: str\n",
    "    model_basename: str\n",
    "    preload: str\n",
    "    tokenizer_file: str\n",
    "    experiment_name: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Translate.constants import *\n",
    "import os\n",
    "from Translate.utils.common import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath = CONFIG_FILE_PATH,\n",
    "        params_filepath = PARAMS_FILE_PATH):\n",
    "\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "    \n",
    "\n",
    "    def get_config(self) -> Config_Data:\n",
    "        config = self.config.config_data\n",
    "\n",
    "        # create_directories([config.root_dir])\n",
    "\n",
    "        data_ingestion_config = Config_Data(\n",
    "            \n",
    "            batch_size = config.batch_size,\n",
    "            num_epochs = config.num_epochs,\n",
    "            lr = config.lr,\n",
    "            seq_len =  config.seq_len,\n",
    "            d_model = config.d_model,\n",
    "            datasource = config.datasource,\n",
    "            lang_src = config.lang_src,\n",
    "            lang_tgt = config.lang_tgt,\n",
    "            model_folder = config.model_folder,\n",
    "            model_basename = config.model_basename,\n",
    "            preload = config.preload,\n",
    "            tokenizer_file = config.tokenizer_file,\n",
    "            experiment_name = config.experiment_name,\n",
    "        )\n",
    "\n",
    "        return data_ingestion_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/nikhil0035/Documents/GitHub/Machine_Translation_using_Transformers\n"
     ]
    }
   ],
   "source": [
    "os.chdir('../')\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/nikhil0035/Documents/GitHub/Machine_Translation_using_Transformers\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nikhil0035/anaconda3/envs/tanslaion/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from Translate import logger\n",
    "from Translate.entity.config_entity import Config_Data\n",
    "from Translate.components.dataset import BilingualDataset\n",
    "\n",
    "from datasets import load_dataset\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordLevel\n",
    "from tokenizers.trainers import WordLevelTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "class DataInjestion():\n",
    "    def __init__(self,config: Config_Data,data_class):\n",
    "        self.config = config\n",
    "        self.BilingualDataset = data_class\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_all_sentences(ds, lang):\n",
    "        for item in ds:\n",
    "            yield item['translation'][lang]\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_or_build_tokenizer(config,ds, lang):\n",
    "        tokenizer_path = Path(config.tokenizer_file.format(lang))\n",
    "        if not Path.exists(tokenizer_path):\n",
    "\n",
    "            tokenizer = Tokenizer(WordLevel(unk_token=\"[UNK]\"))\n",
    "            tokenizer.pre_tokenizer = Whitespace()\n",
    "            trainer = WordLevelTrainer(special_tokens=[\"[UNK]\", \"[PAD]\", \"[SOS]\", \"[EOS]\"], min_frequency=2)\n",
    "            tokenizer.train_from_iterator(DataInjestion.get_all_sentences(ds, lang), trainer=trainer)\n",
    "            tokenizer.save(str(tokenizer_path))\n",
    "        \n",
    "        else:\n",
    "            tokenizer = Tokenizer.from_file(str(tokenizer_path))\n",
    "        return tokenizer\n",
    "    \n",
    "    \n",
    "    def get_ds(self):\n",
    "        ds_raw = load_dataset(f\"{self.config.datasource}\", f\"{self.config.lang_src}-{self.config.lang_tgt}\", split='train')\n",
    "\n",
    "        tokenizer_src = self.get_or_build_tokenizer(self.config, ds_raw, self.config.lang_src)\n",
    "        tokenizer_tgt = self.get_or_build_tokenizer(self.config, ds_raw, self.config.lang_tgt)\n",
    "\n",
    "        # tokenizer_src = self.get_or_build_tokenizer(ds_raw, self.config.lang_src)\n",
    "        # tokenizer_tgt = self.get_or_build_tokenizer(ds_raw, self.config.lang_tgt)\n",
    "\n",
    "        train_ds_size = int(0.9 * len(ds_raw))\n",
    "        val_ds_size = len(ds_raw) - train_ds_size\n",
    "        train_ds_raw, val_ds_raw = random_split(ds_raw, [train_ds_size, val_ds_size])\n",
    "\n",
    "        train_ds = BilingualDataset(train_ds_raw, tokenizer_src, tokenizer_tgt, self.config.lang_src, self.config.lang_tgt, self.config.seq_len)\n",
    "        val_ds = BilingualDataset(val_ds_raw, tokenizer_src, tokenizer_tgt,  self.config.lang_src, self.config.lang_tgt, self.config.seq_len)\n",
    "\n",
    "        max_len_src = 0\n",
    "        max_len_tgt = 0\n",
    "\n",
    "        for item in ds_raw:\n",
    "            src_ids = tokenizer_src.encode(item['translation'][self.config.lang_src]).ids\n",
    "            tgt_ids = tokenizer_tgt.encode(item['translation'][self.config.lang_tgt]).ids\n",
    "            max_len_src = max(max_len_src, len(src_ids))\n",
    "            max_len_tgt = max(max_len_tgt, len(tgt_ids))\n",
    "\n",
    "        # print(f'Max length of source sentence: {max_len_src}')\n",
    "        # print(f'Max length of target sentence: {max_len_tgt}')\n",
    "        logger.info(f'Max length of source sentence: {max_len_src}')\n",
    "        logger.info(f'Max length of target sentence: {max_len_tgt}')\n",
    "\n",
    "        train_dataloader = DataLoader(train_ds, batch_size=self.config.batch_size, shuffle=True)\n",
    "        val_dataloader = DataLoader(val_ds, batch_size=1, shuffle=True)\n",
    "\n",
    "        return train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-01-28 23:00:10,175: INFO: common: yaml file: config/config.yaml loaded successfully]\n",
      "[2024-01-28 23:00:10,176: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2024-01-28 23:00:10,177: INFO: common: created directory at: artifacts]\n"
     ]
    }
   ],
   "source": [
    "config_instance = ConfigurationManager()\n",
    "config_obj = config_instance.get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "datainjestion = DataInjestion(config=config_obj,data_class=BilingualDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-01-28 23:03:07,537: INFO: 3537205078: Max length of source sentence: 309]\n",
      "[2024-01-28 23:03:07,540: INFO: 3537205078: Max length of target sentence: 274]\n"
     ]
    }
   ],
   "source": [
    "train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt= datainjestion.get_ds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"'You know how fond I am of you, Alexis,' she replied when she had heard him out, 'and how ready I am to do anything for you; but I have kept silent because I knew I could be of no use to you and Anna Arkadyevna.' She pronounced the formal 'Anna Arkadyevna' with peculiar precision.\", \"'A decision, some decision, Alexis Alexandrovich!\", \"'So I shall: I am going the day after to-morrow, Agatha Mikhaylovna, only I must finish my business.'\", \"'No, please stay!\", 'I hardly know whether I had slept or not after this musing; at any rate, I started wide awake on hearing a vague murmur, peculiar and lugubrious, which sounded, I thought, just above me.', 'When he came to him, he stood like one amazed, looking at him, turning him first on one side, then on the other; looked at the wound the bullet had made, which it seems was just in his breast, where it had made a hole, and no great quantity of blood had followed; but he had bled inwardly, for he was quite dead.', 'Without stopping the izvoshchik he ran back beside her.', \"Miss Temple had looked down when he first began to speak to her; but she now gazed straight before her, and her face, naturally pale as marble, appeared to be assuming also the coldness and fixity of that material; especially her mouth, closed as if it would have required a sculptor's chisel to open it, and her brow settled gradually into petrified severity.\"]\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_dataloader))\n",
    "\n",
    "# Print the batch or inspect its structure\n",
    "print(batch['src_text'])\n",
    "print(batch['tgt_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['— Tu sai, Aleksej — ella disse, dopo averlo ascoltato — come io ti voglia bene e come sia pronta a fare tutto per te; ma ho taciuto, perché sapevo che non posso essere utile a te e ad Anna Arkad’evna — ella disse, pronunciando con particolare sforzo “Anna Arkad’evna”. — Ti prego di non credere che io voglia biasimare.', '— Una decisione, una qualsiasi decisione, Aleksej Aleksandrovic.', '— Parto proprio domani, Agaf’ja Michajlovna. Bisogna risolvere la faccenda.', '— No, rimanete ancora, vi prego.', 'Non so se mi addormentassi o no; ma a un tratto sentii sopra alla testa un mormorio vago, strano e lugubre, che mi scosse.', 'Quando gli fu vicino rimase com’uomo sbalordito guardando il cadavere, voltandolo prima su un fianco, indi sull’altro, contemplando la ferita che la palla aveva fatto, che sembra lo avesse colpito esattamente nel petto, onde non si vide al di fuori gran copia di sangue, perchè diffuso tutto nell’interno. Raccolti l’arco e le frecce dell’ucciso, tornossene addietro.', 'Senza fermare la vettura, egli le corse dietro.', 'La signorina Temple aveva abbassato gli occhi quando egli aveva preso a parlare, ma ora teneva lo sguardo fisso dinanzi a sé, e il suo volto ordinariamente pallido come il marmo, ne aveva presa la freddezza e la fissità; la bocca specialmente era così chiusa che pareva non avrebbe potuto aprirla altro che lo scalpello dello scultore.']\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tanslaion",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
