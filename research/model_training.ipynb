{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/nikhil0035/Documents/GitHub/Machine_Translation_using_Transformers/research\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/nikhil0035/Documents/GitHub/Machine_Translation_using_Transformers\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "!pwd\n",
    "os.chdir('../')\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class TrainingConfig:\n",
    "    batch_size: int\n",
    "    num_epochs: int\n",
    "    lr: float\n",
    "    seq_len: int\n",
    "    d_model: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nikhil0035/anaconda3/envs/tanslaion/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from Translate.constants import *\n",
    "from Translate.utils.common import *\n",
    "from Translate.entity.config_entity import Config_Data\n",
    "\n",
    "\n",
    "import torchtext.datasets as datasets\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weights_file_path(config, epoch: str):\n",
    "    model_folder = f\"{config.datasource}_{config.model_folder}\"\n",
    "    model_filename = f\"{config.model_basename}{epoch}.pt\"\n",
    "    return str(Path('.') / model_folder / model_filename)\n",
    "\n",
    "# Find the latest weights file in the weights folder\n",
    "def latest_weights_file_path(config):\n",
    "    model_folder = f\"{config.datasource}_{config.model_folder}\"\n",
    "    model_filename = f\"{config.model_basename}*\"\n",
    "    weights_files = list(Path(model_folder).glob(model_filename))\n",
    "    if len(weights_files) == 0:\n",
    "        return None\n",
    "    weights_files.sort()\n",
    "    return str(weights_files[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath = CONFIG_FILE_PATH,\n",
    "        params_filepath = PARAMS_FILE_PATH):\n",
    "\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "    \n",
    "    def get_training_config(self)->TrainingConfig:\n",
    "\n",
    "        params = self.params\n",
    "\n",
    "        training_config = TrainingConfig(\n",
    "\n",
    "            batch_size = params.batch_size,\n",
    "            num_epochs = params.num_epochs,\n",
    "            lr = params.lr,\n",
    "            seq_len =  params.seq_len,\n",
    "            d_model = params.d_model,\n",
    "        )\n",
    "\n",
    "        return training_config\n",
    "\n",
    "    \n",
    "    def get_config(self) -> Config_Data:\n",
    "        config = self.config.config_data\n",
    "\n",
    "        # create_directories([config.root_dir])\n",
    "\n",
    "        data_ingestion_config = Config_Data(\n",
    "            \n",
    "            batch_size = config.batch_size,\n",
    "            num_epochs = config.num_epochs,\n",
    "            lr = config.lr,\n",
    "            seq_len =  config.seq_len,\n",
    "            d_model = config.d_model,\n",
    "            datasource = config.datasource,\n",
    "            lang_src = config.lang_src,\n",
    "            lang_tgt = config.lang_tgt,\n",
    "            model_folder = config.model_folder,\n",
    "            model_basename = config.model_basename,\n",
    "            preload = config.preload,\n",
    "            tokenizer_file = config.tokenizer_file,\n",
    "            experiment_name = config.experiment_name,\n",
    "        )\n",
    "\n",
    "        return data_ingestion_config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-01-31 22:35:23,045: INFO: common: yaml file: config/config.yaml loaded successfully]\n",
      "[2024-01-31 22:35:23,048: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2024-01-31 22:35:23,048: INFO: common: created directory at: artifacts]\n"
     ]
    }
   ],
   "source": [
    "config_obj=ConfigurationManager()\n",
    "param = config_obj.get_training_config()\n",
    "config = config_obj.get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class train_model():\n",
    "    def __init__(self,config,param,train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt,model):\n",
    "        self.param=param\n",
    "        self.config=config\n",
    "        self.train_dataloader=train_dataloader\n",
    "        self.val_dataloader=val_dataloader\n",
    "        self.tokenizer_src=tokenizer_src\n",
    "        self.tokenizer_tgt=tokenizer_tgt\n",
    "        self.model = model\n",
    "    \n",
    "    def train(self):\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.has_mps or torch.backends.mps.is_available() else \"cpu\"\n",
    "        print(\"Using device:\", device)\n",
    "        if (device == 'cuda'):\n",
    "            print(f\"Device name: {torch.cuda.get_device_name(device.index)}\")\n",
    "            print(f\"Device memory: {torch.cuda.get_device_properties(device.index).total_memory / 1024 ** 3} GB\")\n",
    "        elif (device == 'mps'):\n",
    "            print(f\"Device name: <mps>\")\n",
    "        else:\n",
    "            print(\"NOTE: If you have a GPU, consider using it for training.\")\n",
    "            print(\"      On a Windows machine with NVidia GPU, check this video: https://www.youtube.com/watch?v=GMSjDTU8Zlc\")\n",
    "            print(\"      On a Mac machine, run: pip3 install --pre torch torchvision torchaudio torchtext --index-url https://download.pytorch.org/whl/nightly/cpu\")\n",
    "        device = torch.device(device)\n",
    "\n",
    "        Path(f\"{config.datasource}_{config.model_folder}\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        model = self.model.to(device)\n",
    "        \n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.001, eps=1e-9)\n",
    "        \n",
    "        initial_epoch = 0\n",
    "        global_step = 0\n",
    "        preload = config.preload\n",
    "\n",
    "        model_filename = latest_weights_file_path(config) if preload == 'latest' else get_weights_file_path(config, preload) if preload else None\n",
    "\n",
    "        if model_filename:\n",
    "            print(f'Preloading model {model_filename}')\n",
    "            state = torch.load(model_filename)\n",
    "            model.load_state_dict(state['model_state_dict'])\n",
    "            initial_epoch = state['epoch'] + 1\n",
    "            optimizer.load_state_dict(state['optimizer_state_dict'])\n",
    "            global_step = state['global_step']\n",
    "        else:\n",
    "            print('No model to preload, starting from scratch')\n",
    "        \n",
    "        loss_fn = nn.CrossEntropyLoss(ignore_index=self.tokenizer_src.token_to_id('[PAD]'), label_smoothing=0.1).to(device)\n",
    "\n",
    "        for epoch in range(initial_epoch, param.num_epochs):\n",
    "            torch.cuda.empty_cache()\n",
    "            model.train()\n",
    "            batch_iterator = tqdm(self.train_dataloader, desc=f\"Processing Epoch {epoch:02d}\")\n",
    "\n",
    "            for batch in batch_iterator:\n",
    "\n",
    "                encoder_input = batch['encoder_input'].to(device) # (b, seq_len)\n",
    "                decoder_input = batch['decoder_input'].to(device) # (B, seq_len)\n",
    "                encoder_mask = batch['encoder_mask'].to(device) # (B, 1, 1, seq_len)\n",
    "                decoder_mask = batch['decoder_mask'].to(device) # (B, 1, seq_len, seq_len)\n",
    "\n",
    "                # Run the tensors through the encoder, decoder and the projection layer\n",
    "                encoder_output = model.encode(encoder_input, encoder_mask) # (B, seq_len, d_model)\n",
    "                decoder_output = model.decode(encoder_output, encoder_mask, decoder_input, decoder_mask) # (B, seq_len, d_model)\n",
    "                proj_output = model.project(decoder_output) # (B, seq_len, vocab_size)\n",
    "\n",
    "                # Compare the output with the label\n",
    "                label = batch['label'].to(device) # (B, seq_len)\n",
    "\n",
    "                # Compute the loss using a simple cross entropy\n",
    "                loss = loss_fn(proj_output.view(-1, self.tokenizer_tgt.get_vocab_size()), label.view(-1))\n",
    "                batch_iterator.set_postfix({\"loss\": f\"{loss.item():6.3f}\"})\n",
    "\n",
    "                # # Log the loss\n",
    "                # writer.add_scalar('train loss', loss.item(), global_step)\n",
    "                # writer.flush()\n",
    "\n",
    "                # Backpropagate the loss\n",
    "                loss.backward()\n",
    "\n",
    "                # Update the weights\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "                global_step += 1\n",
    "\n",
    "            model_filename = get_weights_file_path(config, f\"{epoch:02d}\")\n",
    "            torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'global_step': global_step\n",
    "            }, model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Translate.pipeline.stage_01_data_injestion import DataIngestionTrainingPipeline\n",
    "from Translate.pipeline.stage_02_prepare_model import PrepareModelPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-01-31 22:35:23,081: INFO: common: yaml file: config/config.yaml loaded successfully]\n",
      "[2024-01-31 22:35:23,082: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2024-01-31 22:35:23,082: INFO: common: created directory at: artifacts]\n",
      "[2024-01-31 22:35:28,455: INFO: data_injestion: Max length of source sentence: 309]\n",
      "[2024-01-31 22:35:28,455: INFO: data_injestion: Max length of target sentence: 274]\n"
     ]
    }
   ],
   "source": [
    "data_obj = DataIngestionTrainingPipeline()\n",
    "train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt = data_obj.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-01-31 22:35:28,461: INFO: common: yaml file: config/config.yaml loaded successfully]\n",
      "[2024-01-31 22:35:28,462: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2024-01-31 22:35:28,463: INFO: common: created directory at: artifacts]\n"
     ]
    }
   ],
   "source": [
    "prepare_model_obj = PrepareModelPipeline()\n",
    "model = prepare_model_obj.main(tokenizer_src.get_vocab_size(), tokenizer_tgt.get_vocab_size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_obj = train_model(config,param,train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "Device name: <mps>\n",
      "No model to preload, starting from scratch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 00:   0%|          | 0/29098 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "train_obj.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tanslaion",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
